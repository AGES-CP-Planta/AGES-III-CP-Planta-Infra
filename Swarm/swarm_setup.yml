- name: Configure the Docker Swarm Cluster with Optimized Workload Distribution
  hosts: all
  become: yes
  vars:
    manager_node: "{{ groups['instance1'][0] }}"
    first_worker: "{{ groups['instance2'][0] }}"
    ansible_python_interpreter: /usr/bin/python3

  tasks:
  # Debug task to help troubleshoot variable issues
  - name: Debug inventory and host information
    debug:
      msg:
        - "Current host: {{ inventory_hostname }}"
        - "Manager node: {{ manager_node }}"
        - "Is manager: {{ inventory_hostname == manager_node }}"
        - "Manager IP: {{ hostvars[manager_node].ansible_host | default(manager_node) }}"
        
  # Docker setup tasks remain the same
  - name: Remove old Docker versions
    apt:
      name:
        - docker
        - docker-engine
        - docker.io
        - containerd
        - runc
      state: absent
      update_cache: yes

  - name: Install dependencies
    apt:
      name:
        - curl
        - ca-certificates
        - git
        - acl
        - python3
        - python3-pip
        - lsb-release
      state: present
      update_cache: yes

  - name: Add Docker's official GPG key and save it in the recommended keyring
    ansible.builtin.shell: |
      curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    args:
      creates: /usr/share/keyrings/docker-archive-keyring.gpg
    ignore_errors: yes

  - name: Add Docker's repository
    ansible.builtin.shell: |
      echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu jammy stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    ignore_errors: yes

  - name: Update apt cache
    ansible.builtin.apt:
      update_cache: yes

  - name: Install Docker packages
    apt:
      name:
        - docker-ce
        - docker-ce-cli
        - containerd.io
      state: present
      update_cache: yes

  - name: Install Docker SDK for Python
    apt:
      name: python3-docker
      state: present

  - name: Add users to the Docker group
    user:
      name: "{{ item }}"
      groups: docker
      append: yes
    loop:
      - "{{ ansible_ssh_user }}"
    ignore_errors: yes

  - name: Ensure Docker service is running
    systemd:
      name: docker
      state: started
      enabled: true

  # Configure Docker daemon with proper resource settings
  - name: Create Docker daemon.json directory
    file:
      path: /etc/docker
      state: directory
      mode: '0755'

  - name: Configure Docker daemon
    copy:
      content: |
        {
          "log-driver": "json-file",
          "log-opts": {
            "max-size": "10m",
            "max-file": "3"
          },
          "default-address-pools": [
            {
              "base": "172.80.0.0/16",
              "size": 24
            }
          ],
          "metrics-addr": "0.0.0.0:9323",
          "experimental": true
        }
      dest: /etc/docker/daemon.json
    register: docker_config

  - name: Restart Docker service if config changed
    systemd:
      name: docker
      state: restarted
    when: docker_config.changed

  # Store the manager's IP address for later use
  - name: Set manager IP fact on manager node
    set_fact:
      manager_ip: "{{ ansible_default_ipv4.address }}"
    when: inventory_hostname == manager_node

  # Check if node is already part of a swarm
  - name: Check if swarm is already initialized
    shell: docker info | grep "Swarm" | awk '{print $2}'
    register: swarm_status
    ignore_errors: yes
    changed_when: false

  # Initialize swarm on manager node if not already in a swarm
  - name: Initialize Swarm on manager node
    docker_swarm:
      state: present
      advertise_addr: "{{ ansible_default_ipv4.address }}"
    when: inventory_hostname == manager_node and swarm_status.stdout != "active"
    register: swarm_init

  # Distribute manager IP to all hosts
  - name: Get manager IP
    set_fact:
      manager_ip: "{{ hostvars[manager_node]['manager_ip'] }}"

  # Get swarm join tokens from manager node
  - name: Get swarm join tokens
    docker_swarm_info:
    when: inventory_hostname == manager_node
    register: swarm_info

  # Make the join token available to worker nodes
  - name: Set worker token fact
    set_fact:
      worker_token: "{{ hostvars[manager_node]['swarm_info']['swarm_facts']['JoinTokens']['Worker'] }}"
    when: 
      - inventory_hostname != manager_node
      - hostvars[manager_node]['swarm_info'] is defined
      - hostvars[manager_node]['swarm_info']['swarm_facts'] is defined

  # Join worker nodes to the swarm
  - name: Join worker nodes to the swarm
    docker_swarm:
      state: present
      join_token: "{{ worker_token }}"
      remote_addrs: [ "{{ manager_ip }}" ]
    when: 
      - inventory_hostname != manager_node
      - worker_token is defined
      - swarm_status.stdout != "active"

  # Copy DNS and stack configuration files 
  - name: Copy stack.yml to manager
    copy:
      src: ./stack.yml
      dest: /home/{{ ansible_ssh_user }}/stack.yml
    delegate_to: "{{ manager_node }}"
    run_once: true 
    
  # Copy DNS configuration files
  - name: Create DNS directory structure on manager
    file:
      path: /home/{{ ansible_ssh_user }}/dns/zones
      state: directory
      mode: '0755'
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Copy Corefile to manager
    copy:
      src: ./Corefile
      dest: /home/{{ ansible_ssh_user }}/dns/Corefile
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Copy zone file to manager
    copy:
      src: ./cpplanta.duckdns.org.db
      dest: /home/{{ ansible_ssh_user }}/dns/zones/cpplanta.duckdns.org.db
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Update zone file with manager IP
    replace:
      path: /home/{{ ansible_ssh_user }}/dns/zones/cpplanta.duckdns.org.db
      regexp: '10\.0\.1\.10'
      replace: "{{ manager_ip }}"
    delegate_to: "{{ manager_node }}"
    run_once: true

  # Check if node is initialized as swarm manager before proceeding
  - name: Check if manager is initialized
    shell: docker node ls
    register: swarm_nodes_check
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes

  # Get Docker node IDs after ensuring swarm is initialized
  - name: Get Docker node IDs and details
    shell: docker node ls --format '{% raw %}{{.ID}}\t{{.Hostname}}\t{{.Status}}\t{{.Availability}}\t{{.ManagerStatus}}{% endraw %}'
    register: node_details
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes
    when: swarm_nodes_check is success

  - name: Debug node details
    debug:
      var: node_details
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes
    when: node_details is defined

  - name: Parse node details
    set_fact:
      swarm_nodes: "{{ node_details.stdout_lines | map('split', '\t') | list }}"
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes
    when: node_details is defined and node_details.stdout_lines is defined

  - name: Identify manager node
    set_fact:
      manager_node_id: "{{ swarm_nodes | selectattr('4', 'search', 'Leader|Reachable') | map('first') | first }}"
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes
    when: swarm_nodes is defined

  - name: Identify worker nodes
    set_fact:
      worker_node_ids: "{{ swarm_nodes | rejectattr('4', 'search', 'Leader|Reachable') | map('first') | list }}"
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes
    when: swarm_nodes is defined

  - name: Debug manager and worker node IDs
    debug:
      msg:
        - "Manager node ID: {{ manager_node_id | default('Not identified yet') }}"
        - "Worker node IDs: {{ worker_node_ids | default('Not identified yet') }}"
    delegate_to: "{{ manager_node }}"
    run_once: true
    ignore_errors: yes

  # Deploy the stack with better resource allocation
  - name: Deploy the stack on manager (with force update)
    shell: docker stack deploy --with-registry-auth --resolve-image always -c /home/{{ ansible_ssh_user }}/stack.yml CP-Planta
    args:
      chdir: /home/{{ ansible_ssh_user }}
    delegate_to: "{{ manager_node }}"
    run_once: true

  # Install monitoring tools
  - name: Install monitoring tools (htop, ctop, etc)
    apt:
      name:
        - htop
        - iotop
        - sysstat
      state: present
    become: yes

  - name: Install ctop (container monitoring tool)
    shell: |
      wget -qO- https://azlux.fr/repo.gpg.key | sudo apt-key add -
      echo "deb http://packages.azlux.fr/debian/ bullseye main" | sudo tee /etc/apt/sources.list.d/azlux.list
      sudo apt update
      sudo apt install -y ctop
    args:
      creates: /usr/bin/ctop
    become: yes
    ignore_errors: yes

  - name: Wait for 25 seconds to show proper service status
    wait_for:
      timeout: 25

  - name: Show service status
    command: docker service ls
    delegate_to: "{{ manager_node }}"
    run_once: true
    register: service_status
    ignore_errors: yes
  
  - name: Display service status
    debug:
      var: service_status.stdout_lines
    when: service_status is defined

  - name: Show node resource utilization
    command: docker node ls
    delegate_to: "{{ manager_node }}"
    run_once: true
    register: node_status
    ignore_errors: yes
  
  - name: Display node status
    debug:
      var: node_status.stdout_lines
    when: node_status is defined