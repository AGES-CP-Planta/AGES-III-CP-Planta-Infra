- name: Configure the Docker Swarm Cluster
  hosts: all
  become: yes
  vars:
    manager_node: "{{ groups['instance1'][0] }}"
    first_worker: "{{ groups['instance2'][0] }}"
    ansible_python_interpreter: /usr/bin/python3

  tasks:
  # Debug task to help troubleshoot variable issues
  - name: Debug inventory and host information
    debug:
      msg:
        - "Current host: {{ inventory_hostname }}"
        - "Manager node: {{ manager_node }}"
        - "Is manager: {{ inventory_hostname == manager_node }}"
        - "Manager IP: {{ hostvars[manager_node].ansible_host | default(manager_node) }}"
        - "Worker IP: {{ hostvars[first_worker].ansible_host | default(first_worker) }}"
        
  # Docker setup tasks
  - name: Remove old Docker versions
    apt:
      name:
        - docker
        - docker-engine
        - docker.io
        - containerd
        - runc
      state: absent
      update_cache: yes

  - name: Install dependencies
    apt:
      name:
        - curl
        - ca-certificates
        - git
        - acl
        - python3
        - python3-pip
        - lsb-release
        - wget
        - netcat-openbsd
        - dnsutils
      state: present
      update_cache: yes

  # Install monitoring tools
  - name: Install monitoring tools
    apt:
      name:
        - htop
        - iotop
        - sysstat
      state: present
    become: yes

  - name: Check if ctop is already installed
    stat:
      path: /usr/local/bin/ctop
    register: ctop_installed

  - name: Get latest ctop release from GitHub API
    uri:
      url: https://api.github.com/repos/bcicen/ctop/releases/latest
      return_content: yes
    register: ctop_release
    when: not ctop_installed.stat.exists

  - name: Install ctop from latest GitHub release
    get_url:
      url: "https://github.com/bcicen/ctop/releases/download/{{ ctop_release.json.tag_name }}/ctop-{{ ctop_release.json.tag_name | regex_replace('^v', '') }}-linux-amd64"
      dest: /usr/local/bin/ctop
      mode: '0755'
    become: yes
    when: not ctop_installed.stat.exists

  - name: Add Docker's official GPG key and save it in the recommended keyring
    ansible.builtin.shell: |
      curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    args:
      creates: /usr/share/keyrings/docker-archive-keyring.gpg
    ignore_errors: yes

  - name: Add Docker's repository
    ansible.builtin.shell: |
      echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu jammy stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    ignore_errors: yes

  - name: Update apt cache
    ansible.builtin.apt:
      update_cache: yes

  - name: Install Docker packages
    apt:
      name:
        - docker-ce
        - docker-ce-cli
        - containerd.io
      state: present
      update_cache: yes

  - name: Install Docker SDK for Python
    apt:
      name: python3-docker
      state: present

  - name: Add users to the Docker group
    user:
      name: "{{ item }}"
      groups: docker
      append: yes
    loop:
      - "{{ ansible_ssh_user }}"
    ignore_errors: yes

  - name: Ensure Docker service is running
    systemd:
      name: docker
      state: started
      enabled: true

  # Initialize swarm on manager
  - name: Initialize Swarm on manager node
    shell: docker swarm init --advertise-addr {{ inventory_hostname }}
    when: inventory_hostname == manager_node
    register: swarm_init
    ignore_errors: yes

  # Debug the init output
  - name: Debug swarm init
    debug:
      var: swarm_init
    when: inventory_hostname == manager_node

  # Get the worker token separately
  - name: Get swarm worker token
    shell: docker swarm join-token -q worker
    register: token_output
    when: inventory_hostname == manager_node

  # Debug the token output
  - name: Debug token output
    debug:
      var: token_output
    when: inventory_hostname == manager_node

  # Set token as a fact on manager
  - name: Set token fact on manager
    set_fact:
      worker_token: "{{ token_output.stdout }}"
    when: inventory_hostname == manager_node

  # Join workers to swarm using the token
  - name: Join workers to swarm
    shell: "docker swarm join --token {{ hostvars[groups['instance1'][0]].worker_token }} {{ groups['instance1'][0] }}:2377"
    when: inventory_hostname != manager_node
    ignore_errors: yes

  # Create necessary directories on manager
  - name: Create DNS directory structure on manager
    file:
      path: /home/{{ ansible_ssh_user }}/dns/zones
      state: directory
      mode: '0755'
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Create database directory structure on manager
    file:
      path: /home/{{ ansible_ssh_user }}/database/postgresql
      state: directory
      mode: '0755'
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Create database directory structure for pgbouncer on manager
    file:
      path: /home/{{ ansible_ssh_user }}/database/pgbouncer
      state: directory
      mode: '0755'
    delegate_to: "{{ manager_node }}"
    run_once: true

  # Copy configuration files
  - name: Copy stack.yml to manager
    copy:
      src: ../../swarm/stack.yml
      dest: /home/{{ ansible_ssh_user }}/stack.yml
    delegate_to: "{{ manager_node }}"
    run_once: true 

  - name: Copy Corefile to manager
    copy:
      src: ../roles/networking/Corefile
      dest: /home/{{ ansible_ssh_user }}/dns/Corefile
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Copy PostgreSQL pg_hba.conf to manager
    copy:
      src: ../roles/database/postgresql/pg_hba.conf
      dest: /home/{{ ansible_ssh_user }}/database/postgresql/pg_hba.conf
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Copy PgBouncer configuration to manager
    copy:
      src: ../roles/database/pgbouncer/pgbouncer.ini
      dest: /home/{{ ansible_ssh_user }}/database/pgbouncer/pgbouncer.ini
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Copy PgBouncer userlist to manager
    copy:
      src: ../roles/database/pgbouncer/userlist.txt
      dest: /home/{{ ansible_ssh_user }}/database/pgbouncer/userlist.txt
    delegate_to: "{{ manager_node }}"
    run_once: true

  # Update IP addresses in configuration files
  - name: Get manager node IP address
    shell: hostname -I | awk '{print $1}'
    register: manager_ip
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Get worker node IP address
    shell: hostname -I | awk '{print $1}'
    register: worker_ip
    delegate_to: "{{ first_worker }}"
    run_once: true

  - name: Update Corefile with actual IP addresses
    replace:
      path: /home/{{ ansible_ssh_user }}/dns/Corefile
      regexp: '10\.0\.1\.10'
      replace: "{{ manager_ip.stdout }}"
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Update Corefile with worker IP address
    replace:
      path: /home/{{ ansible_ssh_user }}/dns/Corefile
      regexp: '10\.0\.1\.20'
      replace: "{{ worker_ip.stdout }}"
    delegate_to: "{{ manager_node }}"
    run_once: true

  # Create a healthcheck script on manager node
  - name: Create healthcheck script on manager
    copy:
      dest: /home/{{ ansible_ssh_user }}/healthcheck.sh
      content: |
        #!/bin/bash
        # Simple healthcheck script to verify service status
        
        check_service() {
          local service=$1
          local status=$(docker service ls --format '{{.Name}} {{.Replicas}}' | grep $service)
          if [[ "$status" == *"0/1"* ]]; then
            echo "$service is not running correctly"
            return 1
          else
            echo "$service is running"
            return 0
          fi
        }
        
        # Check core services
        services=("CP-Planta_traefik" "CP-Planta_dns" "CP-Planta_postgres_primary" "CP-Planta_pgbouncer")
        
        for service in "${services[@]}"; do
          if ! check_service $service; then
            echo "Restarting $service..."
            docker service update --force $service
            sleep 20
          fi
        done
        
        # Final report
        docker service ls
      mode: '0755'
    delegate_to: "{{ manager_node }}"
    run_once: true

  # Deploy the stack
  - name: Deploy the stack on manager
    shell: docker stack deploy --with-registry-auth -c /home/{{ ansible_ssh_user }}/stack.yml CP-Planta
    args:
      chdir: /home/{{ ansible_ssh_user }}
    delegate_to: "{{ manager_node }}"
    run_once: true

  - name: Wait for services to start up
    pause:
      seconds: 60

  # Check service status
  - name: Check service status
    command: docker service ls
    register: service_status
    delegate_to: "{{ manager_node }}"
    run_once: true

  # First start core services if they're not running
  - name: Start DNS service if not running
    shell: docker service update --force CP-Planta_dns
    delegate_to: "{{ manager_node }}"
    run_once: true
    when: "'0/1' in service_status.stdout and 'CP-Planta_dns' in service_status.stdout"

  - name: Wait for DNS to become available
    pause:
      seconds: 30
    when: "'0/1' in service_status.stdout and 'CP-Planta_dns' in service_status.stdout"

  # Now start database services
  - name: Start PostgreSQL service if not running
    shell: docker service update --force CP-Planta_postgres_primary
    delegate_to: "{{ manager_node }}"
    run_once: true
    when: "'0/1' in service_status.stdout and 'CP-Planta_postgres_primary' in service_status.stdout"

  - name: Wait for PostgreSQL to become available
    pause:
      seconds: 30
    when: "'0/1' in service_status.stdout and 'CP-Planta_postgres_primary' in service_status.stdout"

  # Then start PgBouncer
  - name: Start PgBouncer service if not running
    shell: docker service update --force CP-Planta_pgbouncer
    delegate_to: "{{ manager_node }}"
    run_once: true
    when: "'0/1' in service_status.stdout and 'CP-Planta_pgbouncer' in service_status.stdout"

  - name: Wait for PgBouncer to become available
    pause:
      seconds: 30
    when: "'0/1' in service_status.stdout and 'CP-Planta_pgbouncer' in service_status.stdout"

  # Finally start application services
  - name: Start backend service if not running
    shell: docker service update --force CP-Planta_backend
    delegate_to: "{{ manager_node }}"
    run_once: true
    when: "'0/1' in service_status.stdout and 'CP-Planta_backend' in service_status.stdout"

  - name: Start frontend service if not running
    shell: docker service update --force CP-Planta_frontend
    delegate_to: "{{ manager_node }}"
    run_once: true
    when: "'0/1' in service_status.stdout and 'CP-Planta_frontend' in service_status.stdout"

  # Get final status
  - name: Show service status
    command: docker service ls
    delegate_to: "{{ manager_node }}"
    run_once: true
    register: final_service_status
    ignore_errors: yes
  
  - name: Display service status
    debug:
      var: final_service_status.stdout_lines
    when: final_service_status is defined

  # Add a cron job to run the healthcheck script periodically
  - name: Add healthcheck cron job on manager
    cron:
      name: "Docker Swarm Service Healthcheck"
      minute: "*/15"
      job: "/home/{{ ansible_ssh_user }}/healthcheck.sh >> /home/{{ ansible_ssh_user }}/healthcheck.log 2>&1"
    delegate_to: "{{ manager_node }}"
    run_once: true